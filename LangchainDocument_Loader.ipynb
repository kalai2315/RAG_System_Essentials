{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPB0vP3YxzR1LQ5wp4LVir3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalai2315/RAG_System_Essentials/blob/main/LangchainDocument_Loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Document Loaders in LangChain"
      ],
      "metadata": {
        "id": "IIrvwQSxNudH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhXnRjz7No-M"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install -U --quiet langchain langchain-google-genai langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[all-docs]==0.14.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilsLscv1ObJo",
        "outputId": "3804e615-cb25-447b-bb17-04421a55aa6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured==0.14.0 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.14.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.9.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.13.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.0.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.13.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.14.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.37.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.17.2)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (9.9.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (20250506)\n",
            "Requirement already satisfied: python-pptx<=0.6.23 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.6.23)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.5)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.17.0)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.15)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.18.0)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.3.15)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.10.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.2.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (5.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (2.2.2)\n",
            "Requirement already satisfied: pillow-heif in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.22.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.8.2)\n",
            "Requirement already satisfied: msg-parser in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (1.2.0)\n",
            "Requirement already satisfied: unstructured-inference==0.7.31 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (0.7.31)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (3.1.5)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]==0.14.0) (2.0.2)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.33.0)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.11.0.86)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.22.0)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.52.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]==0.14.0) (11.2.1)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]==0.14.0) (3.2.5)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]==0.14.0) (24.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]==0.14.0) (5.29.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.17.0)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.11/dist-packages (from msg-parser->unstructured[all-docs]==0.14.0) (0.47)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->unstructured[all-docs]==0.14.0) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]==0.14.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[all-docs]==0.14.0) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[all-docs]==0.14.0) (43.0.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pikepdf->unstructured[all-docs]==0.14.0) (1.2.18)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2025.6.15)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (24.1.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]==0.14.0) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.16.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.1.5)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.15.3)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.11.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.4.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.3.13)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]==0.14.0) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]==0.14.0) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured==0.14.0->unstructured[all-docs]==0.14.0) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (10.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.0.15)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.0.10)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (1.3.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.2.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.30.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (4.9.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]==0.14.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install OCR dependencies for unstructured\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pZsJnKAOkY5",
        "outputId": "407e67eb-3d2b-4e58-8a2a-37a400277943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 0s (423 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jq==1.7.0\n",
        "!pip install pypdf==4.2.0\n",
        "!pip install pymupdf==1.24.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG35YnzEOpta",
        "outputId": "1de28f83-dba6-4043-ceec-70b68f1b0eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jq==1.7.0\n",
            "  Downloading jq-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Downloading jq-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (668 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/668.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m665.6/668.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.1/668.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jq\n",
            "Successfully installed jq-1.7.0\n",
            "Collecting pypdf==4.2.0\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "  Attempting uninstall: pypdf\n",
            "    Found existing installation: pypdf 5.6.1\n",
            "    Uninstalling pypdf-5.6.1:\n",
            "      Successfully uninstalled pypdf-5.6.1\n",
            "Successfully installed pypdf-4.2.0\n",
            "Collecting pymupdf==1.24.4\n",
            "  Downloading PyMuPDF-1.24.4-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.3 (from pymupdf==1.24.4)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.4-cp311-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Document Loaders**\n",
        "\n",
        "Document loaders are used to import data from various sources into LangChain as Document objects. A Document typically includes a piece of text along with its associated metadata.\n",
        "\n",
        "# Examples of Document Loaders:\n",
        "\n",
        "Text File Loader: Loads data from a simple .txt file.\n",
        "\n",
        "Web Page Loader: Retrieves the text content from any web page.\n",
        "\n",
        "YouTube Video Transcript Loader: Loads transcripts from YouTube videos.\n",
        "\n",
        "Functionality:\n",
        "\n",
        "Load Method: Each document loader has a load method that enables the loading of data as documents from a pre-configured source.\n",
        "\n",
        "Lazy Load Option: Some loaders also support a \"lazy load\" feature, which allows data to be loaded into memory gradually as needed.\n",
        "\n",
        "For more detailed information, visit LangChain's document loader documentation."
      ],
      "metadata": {
        "id": "gsRiEWXfOwX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o README.md https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaTAeyQaOvvu",
        "outputId": "0ceefe77-9ffd-4424-e8b9-7f0ac4a042c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5169  100  5169    0     0  23203      0 --:--:-- --:--:-- --:--:-- 23283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./README.md\")\n",
        "doc = loader.load()"
      ],
      "metadata": {
        "id": "UWTyL615Qbwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpaiTk8GQfYF",
        "outputId": "6bb28a3a-f0ed-4477-cda7-fe0dd259f8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "hkBuBC7BQipW",
        "outputId": "0c0b75dc-d401-4260-b936-1ee69fc17520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 255);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[0].page_content[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gUjsKA1QpUw",
        "outputId": "c9ce8aac-a196-403e-9f59-3d1a8f3f1da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<picture>\n",
            "  <source media=\"(prefers-color-scheme: light)\" srcset=\"docs/static/img/logo-dark.svg\">\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Markdown Loader**\n",
        "\n",
        "Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\n",
        "\n",
        "This showcases how to load Markdown documents into a langchain document format that we can use in our pipelines and chains."
      ],
      "metadata": {
        "id": "a1Q4pyrJQ608"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2M32RoRQ0Ka",
        "outputId": "75eab87f-dc46-4492-bd44-6d2e077a5e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "loader = UnstructuredMarkdownLoader(\"./README.md\", mode='single')\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp1GfK3ZRBUg",
        "outputId": "99ca5d8c-de1d-496a-a8ac-b6acbcd3d016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG9LlXBwRGZJ",
        "outputId": "be50ffc0-6d8e-4066-c152-54eed38c6f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "gtBu1UfuRJrC",
        "outputId": "3076f44f-9a38-4db1-ad22-eef72e967a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 255);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYN1gAUMRMBO",
        "outputId": "0a81e409-34e0-4b7e-afb7-9fab52ebd1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!NOTE]\n",
            "Looking for the JS/TS library? Check out LangChain.js.\n",
            "\n",
            "LangChain is a framework for buildin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "loader = UnstructuredMarkdownLoader(\"./README.md\", mode=\"elements\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "mzPNRY-kRN9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz-04qO8RSOO",
        "outputId": "6f549dc7-ed91-4a54-f314-d268f3a91a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnLUSMofRauN",
        "outputId": "36234669-b3d1-425f-b48b-be573944fe14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'ada4984f55e3bfe7057f8abd1b24a809'}, page_content='[!NOTE]\\nLooking for the JS/TS library? Check out LangChain.js.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'd9dd2676fd3ef14eba932c9da5c5636b'}, page_content='LangChain is a framework for building LLM-powered applications. It helps you chain\\ntogether interoperable components and third-party integrations to simplify AI\\napplication development —  all while future-proofing decisions as the underlying\\ntechnology evolves.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': 'd096b8fd4a734bd4645dd41583152f12'}, page_content='bash\\npip install -U langchain'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': 'd096b8fd4a734bd4645dd41583152f12', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'b9bc6693902b26077316240c50b8f2c2'}, page_content='To learn more about LangChain, check out\\nthe docs. If you’re looking for more\\nadvanced customization or agent orchestration, check out\\nLangGraph, our framework for building\\ncontrollable agent workflows.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '6c4906b888bacd02e86fcd62eecb7c4a'}, page_content='Why use LangChain?'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '9dfecbf1c43eadad822524820c29b732'}, page_content='LangChain helps developers build applications powered by LLMs through a standard\\ninterface for models, embeddings, vector stores, and more.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '08aa38139038f600d56be5799cd79770'}, page_content='Use LangChain for:\\n- Real-time data augmentation. Easily connect LLMs to diverse data sources and\\nexternal / internal systems, drawing from LangChain’s vast library of integrations with\\nmodel providers, tools, vector stores, retrievers, and more.\\n- Model interoperability. Swap models in and out as your engineering team\\nexperiments to find the best choice for your application’s needs. As the industry\\nfrontier evolves, adapt quickly — LangChain’s abstractions keep you moving without\\nlosing momentum.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'Title', 'element_id': '8c79a11e4939fe6cc8a0a62187715af6'}, page_content='LangChain’s ecosystem'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'c8ea6b63778256f1ab89b8db63334008'}, page_content='While the LangChain framework can be used standalone, it also integrates seamlessly\\nwith any LangChain product, giving developers a full suite of tools when building LLM\\napplications.'),\n",
              " Document(metadata={'source': './README.md', 'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '47d44e641923360b7dc95ee8c4541cd8'}, page_content='To improve your LLM application development, pair LangChain with:')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter([doc.metadata['category'] for doc in docs])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZZjidGBRfD-",
        "outputId": "30e23700-11e1-450b-be94-c4f3dbd33d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'NarrativeText': 7, 'Title': 4, 'ListItem': 7})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.md import partition_md\n",
        "\n",
        "docs = partition_md(filename=\"./README.md\")"
      ],
      "metadata": {
        "id": "lAF8YJVKSE0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D07nkITRz_z",
        "outputId": "fd76d1f1-e331-436a-bf91-c937efd2ffef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRcLMxnBR3HD",
        "outputId": "a22acd24-bd17-42c9-94c5-09519cdf6aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.NarrativeText at 0x7a1c94e026d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7a1c99fbe790>,\n",
              " <unstructured.documents.elements.Title at 0x7a1c99fbe3d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7a1c99fbf690>,\n",
              " <unstructured.documents.elements.Title at 0x7a1c99fbce90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7a1c99fbf3d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7a1c99fbca90>,\n",
              " <unstructured.documents.elements.Title at 0x7a1c9b426890>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7a1c984ce5d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7a1c984cd8d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Foc9GcWLR5KN",
        "outputId": "caeb5890-0f46-403b-b7d9-c7390958683b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'NarrativeText',\n",
              " 'element_id': 'ada4984f55e3bfe7057f8abd1b24a809',\n",
              " 'text': '[!NOTE]\\nLooking for the JS/TS library? Check out LangChain.js.',\n",
              " 'metadata': {'last_modified': '2025-06-28T15:57:20',\n",
              "  'languages': ['eng'],\n",
              "  'filetype': 'text/markdown',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'README.md'}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1].to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xVBgHolR8PU",
        "outputId": "704f2a3d-114f-4d21-e7ab-dcd8cb8fcb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'NarrativeText',\n",
              " 'element_id': 'd9dd2676fd3ef14eba932c9da5c5636b',\n",
              " 'text': 'LangChain is a framework for building LLM-powered applications. It helps you chain\\ntogether interoperable components and third-party integrations to simplify AI\\napplication development —  all while future-proofing decisions as the underlying\\ntechnology evolves.',\n",
              " 'metadata': {'last_modified': '2025-06-28T15:57:20',\n",
              "  'languages': ['eng'],\n",
              "  'filetype': 'text/markdown',\n",
              "  'file_directory': '.',\n",
              "  'filename': 'README.md'}}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "lc_docs = [Document(page_content=doc.text,\n",
        "                    metadata=doc.metadata.to_dict())\n",
        "              for doc in docs]\n",
        "lc_docs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0yPdNFSSLlC",
        "outputId": "96639fc2-4578-4c77-d25b-e0f12c4c690c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='[!NOTE]\\nLooking for the JS/TS library? Check out LangChain.js.'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='LangChain is a framework for building LLM-powered applications. It helps you chain\\ntogether interoperable components and third-party integrations to simplify AI\\napplication development —  all while future-proofing decisions as the underlying\\ntechnology evolves.'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='bash\\npip install -U langchain'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': 'd096b8fd4a734bd4645dd41583152f12', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='To learn more about LangChain, check out\\nthe docs. If you’re looking for more\\nadvanced customization or agent orchestration, check out\\nLangGraph, our framework for building\\ncontrollable agent workflows.'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='Why use LangChain?'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='LangChain helps developers build applications powered by LLMs through a standard\\ninterface for models, embeddings, vector stores, and more.'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '6c4906b888bacd02e86fcd62eecb7c4a', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='Use LangChain for:\\n- Real-time data augmentation. Easily connect LLMs to diverse data sources and\\nexternal / internal systems, drawing from LangChain’s vast library of integrations with\\nmodel providers, tools, vector stores, retrievers, and more.\\n- Model interoperability. Swap models in and out as your engineering team\\nexperiments to find the best choice for your application’s needs. As the industry\\nfrontier evolves, adapt quickly — LangChain’s abstractions keep you moving without\\nlosing momentum.'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='LangChain’s ecosystem'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='While the LangChain framework can be used standalone, it also integrates seamlessly\\nwith any LangChain product, giving developers a full suite of tools when building LLM\\napplications.'),\n",
              " Document(metadata={'last_modified': '2025-06-28T15:57:20', 'languages': ['eng'], 'parent_id': '8c79a11e4939fe6cc8a0a62187715af6', 'filetype': 'text/markdown', 'file_directory': '.', 'filename': 'README.md'}, page_content='To improve your LLM application development, pair LangChain with:')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CSV Loader**\n",
        "\n",
        "A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n",
        "\n",
        "LangChain implements a CSV Loader that will load CSV files into a sequence of Document objects. Each row of the CSV file is converted to one document."
      ],
      "metadata": {
        "id": "8fzGyIDbS8cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with some dummy real estate data\n",
        "data = {\n",
        "    'Property_ID': [101, 102, 103, 104, 105],\n",
        "    'Address': ['123 Elm St', '456 Oak St', '789 Pine St', '321 Maple St', '654 Cedar St'],\n",
        "    'City': ['Springfield', 'Rivertown', 'Laketown', 'Hillside', 'Sunnyvale'],\n",
        "    'State': ['CA', 'TX', 'FL', 'NY', 'CO'],\n",
        "    'Zip_Code': [98765, 87654, 76543, 65432, 54321],\n",
        "    'Bedrooms': [3, 2, 4, 3, 5],\n",
        "    'Bathrooms': [2, 1, 3, 2, 4],\n",
        "    'Listing_Price': [500000, 350000, 600000, 475000, 750000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('data.csv', index=False)"
      ],
      "metadata": {
        "id": "tAEAtUCOSPCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path=\"./data.csv\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "2A9xiq1lTF0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwNNQP8VTI8o",
        "outputId": "700d963b-4b73-4b33-951b-01c986ebf584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property_ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip_Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nListing_Price: 350000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property_ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip_Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nListing_Price: 600000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property_ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip_Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 475000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property_ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip_Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nListing_Price: 750000')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cfar-GCTL5S",
        "outputId": "23423922-316e-4b6b-9465-3ab15fdd65f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDNFzI6zTQO-",
        "outputId": "ef8ed66c-6dc4-4873-be66-93aac1d788e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Property_ID: 101\n",
            "Address: 123 Elm St\n",
            "City: Springfield\n",
            "State: CA\n",
            "Zip_Code: 98765\n",
            "Bedrooms: 3\n",
            "Bathrooms: 2\n",
            "Listing_Price: 500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSVLoader will accept a csv_args kwarg that supports customization of arguments passed to Python's csv.DictReader. See the csv module documentation for more information of what csv args are supported."
      ],
      "metadata": {
        "id": "Qy1YEzp-TXi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(file_path=\"./data.csv\",\n",
        "                   csv_args={\n",
        "                      \"delimiter\": \",\",\n",
        "                      \"quotechar\": '\"',\n",
        "                      \"fieldnames\": [\"Property ID\", \"Address\", \"City\", \"State\",\n",
        "                                     \"Zip Code\", \"Bedrooms\", \"Bathrooms\", \"Price\"],\n",
        "                   },\n",
        "                  )\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "LffIf9cDTS2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZVwTzfxTbml",
        "outputId": "61224b34-dc7f-4576-a7be-4dfaf9ad7031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property ID: Property_ID\\nAddress: Address\\nCity: City\\nState: State\\nZip Code: Zip_Code\\nBedrooms: Bedrooms\\nBathrooms: Bathrooms\\nPrice: Listing_Price'),\n",
              " Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 500000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nPrice: 350000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nPrice: 600000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 475000'),\n",
              " Document(metadata={'source': './data.csv', 'row': 5}, page_content='Property ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nPrice: 750000')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unstructured.io loads the entire CSV as a single table"
      ],
      "metadata": {
        "id": "ouE9j1NmTjwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredCSVLoader\n",
        "\n",
        "loader = UnstructuredCSVLoader(\"./data.csv\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "jGoH1S4oTeaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoHxpkrKTnnD",
        "outputId": "c9a2eb60-7b7e-45aa-cfe8-8534b5fb4f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF8PCZ9gTqCn",
        "outputId": "367bd7ce-fe4c-4c68-a8f9-be558b0b1465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data.csv'}, page_content='\\n\\n\\nProperty_ID\\nAddress\\nCity\\nState\\nZip_Code\\nBedrooms\\nBathrooms\\nListing_Price\\n\\n\\n101\\n123 Elm St\\nSpringfield\\nCA\\n98765\\n3\\n2\\n500000\\n\\n\\n102\\n456 Oak St\\nRivertown\\nTX\\n87654\\n2\\n1\\n350000\\n\\n\\n103\\n789 Pine St\\nLaketown\\nFL\\n76543\\n4\\n3\\n600000\\n\\n\\n104\\n321 Maple St\\nHillside\\nNY\\n65432\\n3\\n2\\n475000\\n\\n\\n105\\n654 Cedar St\\nSunnyvale\\nCO\\n54321\\n5\\n4\\n750000\\n\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **JSON Loader**\n",
        "\n",
        "JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\n",
        "\n",
        "JSON Lines is a file format where each line is a valid JSON value.\n",
        "\n",
        "LangChain implements a JSONLoader to convert JSON and JSONL data into LangChain Document objects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.\n",
        "\n",
        "It uses the jq python package. Check out this manual for a detailed documentation of the jq syntax."
      ],
      "metadata": {
        "id": "Uu9gf2jaTzPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Sample data dictionary similar to the one you provided but with modified contents\n",
        "data = {\n",
        "    'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_meeting.jpg'},\n",
        "    'is_still_participant': True,\n",
        "    'joinable_mode': {'link': '', 'mode': 1},\n",
        "    'magic_words': [],\n",
        "    'messages': [\n",
        "        {'content': 'See you soon!',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675597571851},\n",
        "        {'content': 'Thanks for the update! See you then.',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675597435669},\n",
        "        {'content': 'Actually, the green one is sold out.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675596277579},\n",
        "        {'content': 'I was hoping to purchase the green one!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675595140251},\n",
        "        {'content': 'I’m really interested in the green one, not the red!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675595109305},\n",
        "        {'content': 'Here’s the $150 for it.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595068468},\n",
        "        {'photos': [{'creation_timestamp': 1675595059,\n",
        "                     'uri': 'image_of_the_item.jpg'}],\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595060730},\n",
        "        {'content': 'It typically sells for at least $200 online',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675595045152},\n",
        "        {'content': 'How much are you asking?',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675594799696},\n",
        "        {'content': 'Good morning! $50 is far too low.',\n",
        "         'sender_name': 'User B',\n",
        "         'timestamp_ms': 1675577876645},\n",
        "        {'content': 'Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!',\n",
        "         'sender_name': 'User A',\n",
        "         'timestamp_ms': 1675549022673}\n",
        "    ],\n",
        "    'participants': [{'name': 'User A'}, {'name': 'User B'}],\n",
        "    'thread_path': 'inbox/User A and User B chat',\n",
        "    'title': 'User A and User B chat'\n",
        "}\n",
        "\n",
        "# Save the modified data to a JSON file\n",
        "with open('chat_data.json', 'w') as file:\n",
        "    json.dump(data, file, indent=4)"
      ],
      "metadata": {
        "id": "8A5YGz2PTsrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load the full data as a single document"
      ],
      "metadata": {
        "id": "QRQ6bBV0UG9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "yyB5GFMKUAQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37abYGlaUID-",
        "outputId": "0282a7fc-f45d-457b-c485-77eb197eb598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktVlNpQkUKhD",
        "outputId": "b0da7be0-0f2e-4434-ecb4-1473d6870457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we are interested in extracting the values under the messages key of the JSON data"
      ],
      "metadata": {
        "id": "py6F64oDUQON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.messages[]',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxZDew56UNAu",
        "outputId": "e8ebb010-019c-4b55-ca8c-4b1abc325ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='{\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='{\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='{\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='{\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='{\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content='{\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='{\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='{\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='{\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='{\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we are interested in extracting the values under the content field within the messages key of the JSON data"
      ],
      "metadata": {
        "id": "HS2j0xqAUfsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = JSONLoader(\n",
        "    file_path='./chat_data.json',\n",
        "    jq_schema='.messages[].content',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzBUa9bKUVnY",
        "outputId": "0298f6e3-95b6-41f4-d679-7ee19b3f9160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='See you soon!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='Thanks for the update! See you then.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='Actually, the green one is sold out.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='I was hoping to purchase the green one!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='I’m really interested in the green one, not the red!'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='Here’s the $150 for it.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content=''),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='It typically sells for at least $200 online'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='How much are you asking?'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='Good morning! $50 is far too low.'),\n",
              " Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF Loaders**\n",
        "\n",
        "Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n",
        "\n",
        "LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your use-case and through experimentation.\n",
        "\n",
        "Here we will see how to load PDF documents into the LangChain Document format"
      ],
      "metadata": {
        "id": "6KsJn3j9UsZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'layoutparser_paper.pdf' 'http://arxiv.org/pdf/2103.15348.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZueIG5njUmSl",
        "outputId": "5a64745a-9708-4d02-b692-970b7b23808e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-28 16:16:36--  http://arxiv.org/pdf/2103.15348.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2103.15348 [following]\n",
            "--2025-06-28 16:16:37--  http://arxiv.org/pdf/2103.15348\n",
            "Reusing existing connection to arxiv.org:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4686220 (4.5M) [application/pdf]\n",
            "Saving to: ‘layoutparser_paper.pdf’\n",
            "\n",
            "layoutparser_paper. 100%[===================>]   4.47M  10.2MB/s    in 0.4s    \n",
            "\n",
            "2025-06-28 16:16:37 (10.2 MB/s) - ‘layoutparser_paper.pdf’ saved [4686220/4686220]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyPDFLoader**\n",
        "Here we load a PDF using pypdf into list of documents, where each document contains the page content and metadata with page number. Typically each PDF page becomes one document"
      ],
      "metadata": {
        "id": "q8a2YiGXU-6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./layoutparser_paper.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEuIKVUiU5Ed",
        "outputId": "789a4f13-530f-44cf-dd56-61755f0d288b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
            "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwWPhIv-VFqs",
        "outputId": "522d5003-46a6-481b-8d05-70fb63c73737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJKrTyjGVIqG",
        "outputId": "f01e04b9-7e6c-47a6-f421-33773aa7abf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'author': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './layoutparser_paper.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n·Character Recognition ·Open Source library ·Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6miQOQ-VVLAC",
        "outputId": "806e1d27-0516-48e4-ab8f-a74be12f126a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser : A Uniﬁed Toolkit for Deep\n",
            "Learning Based Document Image Analysis\n",
            "Zejiang Shen1( \u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
            "Lee4, Jacob Carlson3, and Weining Li5\n",
            "1Allen Institute for AI\n",
            "shannons@allenai.org\n",
            "2Brown University\n",
            "ruochen zhang@brown.edu\n",
            "3Harvard University\n",
            "{melissadell,jacob carlson }@fas.harvard.edu\n",
            "4University of Washington\n",
            "bcgl@cs.washington.edu\n",
            "5University of Waterloo\n",
            "w422li@uwaterloo.ca\n",
            "Abstract. Recent advances in document image analysis (DIA) have been\n",
            "primarily driven by the application of neural networks. Ideally, research\n",
            "outcomes could be easily deployed in production and extended for further\n",
            "investigation. However, various factors like loosely organized codebases\n",
            "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
            "portant innovations by a wide audience. Though there have been on-going\n",
            "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
            "development in disciplines like natural language processing and computer\n",
            "vision, none of them are optimized for challenges in the domain of DIA.\n",
            "This represents a major gap in the existing toolkit, as DIA is central to\n",
            "academic research across a wide range of disciplines in the social sciences\n",
            "and humanities. This paper introduces LayoutParser , an open-source\n",
            "library for streamlining the usage of DL in DIA research and applica-\n",
            "tions. The core LayoutParser library comes with a set of simple and\n",
            "intuitive interfaces for applying and customizing DL models for layout de-\n",
            "tection, character recognition, and many other document processing tasks.\n",
            "To promote extensibility, LayoutParser also incorporates a community\n",
            "platform for sharing both pre-trained models and full document digiti-\n",
            "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
            "lightweight and large-scale digitization pipelines in real-word use cases.\n",
            "The library is publicly available at https://layout-parser.github.io .\n",
            "Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis\n",
            "·Character Recognition ·Open Source library ·Toolkit.\n",
            "1 Introduction\n",
            "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
            "document image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[4].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bulwYj_VNkd",
        "outputId": "9cefc0f7-e39e-46bc-bed3-ebb6b377050e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\n",
            "Table 1: Current layout detection models in the LayoutParser model zoo\n",
            "Dataset Base Model1Large Model Notes\n",
            "PubLayNet [38] F / M M Layouts of modern scientiﬁc documents\n",
            "PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\n",
            "Newspaper [17] F - Layouts of scanned US newspapers from the 20th century\n",
            "TableBank [18] F F Table region on modern scientiﬁc and business document\n",
            "HJDataset [31] F / M - Layouts of history Japanese documents\n",
            "1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\n",
            "vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n",
            "backbones [ 13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [ 28] (F) and Mask\n",
            "R-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n",
            "using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n",
            "zoo in coming months.\n",
            "layout data structures , which are optimized for eﬃciency and versatility. 3) When\n",
            "necessary, users can employ existing or customized OCR models via the uniﬁed\n",
            "API provided in the OCR module . 4)LayoutParser comes with a set of utility\n",
            "functions for the visualization and storage of the layout data. 5) LayoutParser\n",
            "is also highly customizable, via its integration with functions for layout data\n",
            "annotation and model training . We now provide detailed descriptions for each\n",
            "component.\n",
            "3.1 Layout Detection Models\n",
            "InLayoutParser , a layout model takes a document image as an input and\n",
            "generates a list of rectangular boxes for the target content regions. Diﬀerent\n",
            "from traditional methods, it relies on deep convolutional neural networks rather\n",
            "than manually curated rules to identify content regions. It is formulated as an\n",
            "object detection problem and state-of-the-art models like Faster R-CNN [ 28] and\n",
            "Mask R-CNN [ 12] are used. This yields prediction results of high accuracy and\n",
            "makes it possible to build a concise, generalized interface for layout detection.\n",
            "LayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\n",
            "perform layout detection with only four lines of code in Python:\n",
            "1import layoutparser as lp\n",
            "2image = cv2. imread (\" image_file \") # load images\n",
            "3model = lp. Detectron2LayoutModel (\n",
            "4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\n",
            "5layout = model . detect ( image )\n",
            "LayoutParser provides a wealth of pre-trained model weights using various\n",
            "datasets covering diﬀerent languages, time periods, and document types. Due to\n",
            "domain shift [ 7], the prediction performance can notably drop when models are ap-\n",
            "plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n",
            "document structures and layouts vary greatly in diﬀerent domains, it is important\n",
            "to select models trained on a dataset similar to the test samples. A semantic syntax\n",
            "is used for initializing the model weights in LayoutParser , using both the dataset\n",
            "name and model name lp://<dataset-name>/<model-architecture-name> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyMuPDFLoader**\n",
        "This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page. It uses the pymupdf library internally."
      ],
      "metadata": {
        "id": "rlcxfy1XVS9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"./layoutparser_paper.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "RToT5Q3tVP84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72coVNQuVWpx",
        "outputId": "41922551-7177-491a-c7bb-c220d5b81e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGdAylDqVYg7",
        "outputId": "20be49e2-6eb5-4026-907f-4ce6430a93db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': './layoutparser_paper.pdf', 'file_path': './layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 0}, page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5xNKJsJVaGZ",
        "outputId": "15a41709-bab3-4baf-f421-8fa480f990e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'producer': 'pdfTeX-1.40.21',\n",
              " 'creator': 'LaTeX with hyperref',\n",
              " 'creationdate': '2021-06-22T01:27:10+00:00',\n",
              " 'source': './layoutparser_paper.pdf',\n",
              " 'file_path': './layoutparser_paper.pdf',\n",
              " 'total_pages': 16,\n",
              " 'format': 'PDF 1.5',\n",
              " 'title': '',\n",
              " 'author': '',\n",
              " 'subject': '',\n",
              " 'keywords': '',\n",
              " 'moddate': '2021-06-22T01:27:10+00:00',\n",
              " 'trapped': '',\n",
              " 'modDate': 'D:20210622012710Z',\n",
              " 'creationDate': 'D:20210622012710Z',\n",
              " 'page': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcOHfGbnVdZf",
        "outputId": "93341792-cd1a-4c25-fa78-188f65a623d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayoutParser: A Uniﬁed Toolkit for Deep\n",
            "Learning Based Document Image Analysis\n",
            "Zejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
            "Lee4, Jacob Carlson3, and Weining Li5\n",
            "1 Allen Institute for AI\n",
            "shannons@allenai.org\n",
            "2 Brown University\n",
            "ruochen zhang@brown.edu\n",
            "3 Harvard University\n",
            "{melissadell,jacob carlson}@fas.harvard.edu\n",
            "4 University of Washington\n",
            "bcgl@cs.washington.edu\n",
            "5 University of Waterloo\n",
            "w422li@uwaterloo.ca\n",
            "Abstract. Recent advances in document image analysis (DIA) have been\n",
            "primarily driven by the application of neural networks. Ideally, research\n",
            "outcomes could be easily deployed in production and extended for further\n",
            "investigation. However, various factors like loosely organized codebases\n",
            "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
            "portant innovations by a wide audience. Though there have been on-going\n",
            "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
            "development in disciplines like natural language processing and computer\n",
            "vision, none of them are optimized for challenges in the domain of DIA.\n",
            "This represents a major gap in the existing toolkit, as DIA is central to\n",
            "academic research across a wide range of disciplines in the social sciences\n",
            "and humanities. This paper introduces LayoutParser, an open-source\n",
            "library for streamlining the usage of DL in DIA research and applica-\n",
            "tions. The core LayoutParser library comes with a set of simple and\n",
            "intuitive interfaces for applying and customizing DL models for layout de-\n",
            "tection, character recognition, and many other document processing tasks.\n",
            "To promote extensibility, LayoutParser also incorporates a community\n",
            "platform for sharing both pre-trained models and full document digiti-\n",
            "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
            "lightweight and large-scale digitization pipelines in real-word use cases.\n",
            "The library is publicly available at https://layout-parser.github.io.\n",
            "Keywords: Document Image Analysis · Deep Learning · Layout Analysis\n",
            "· Character Recognition · Open Source library · Toolkit.\n",
            "1\n",
            "Introduction\n",
            "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
            "document image analysis (DIA) tasks including document image classiﬁcation [11,\n",
            "arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UnstructuredPDFLoader**\n",
        "Unstructured.io supports a common interface for working with unstructured or semi-structured file formats, such as Markdown or PDF.\n",
        "\n",
        "LangChain's UnstructuredPDFLoader integrates with Unstructured to parse PDF documents into LangChain Document objects.\n",
        "\n",
        "Load PDF as a single document - no complex parsing"
      ],
      "metadata": {
        "id": "DjNc7IbJVleF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'layoutparser_paper.pdf' 'http://arxiv.org/pdf/2103.15348.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9kxiPWkXVRP",
        "outputId": "9883897d-e76c-4939-e9ba-ef6da219593e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-28 16:27:18--  http://arxiv.org/pdf/2103.15348.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2103.15348 [following]\n",
            "--2025-06-28 16:27:19--  http://arxiv.org/pdf/2103.15348\n",
            "Reusing existing connection to arxiv.org:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4686220 (4.5M) [application/pdf]\n",
            "Saving to: ‘layoutparser_paper.pdf’\n",
            "\n",
            "layoutparser_paper. 100%[===================>]   4.47M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-06-28 16:27:19 (68.8 MB/s) - ‘layoutparser_paper.pdf’ saved [4686220/4686220]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "loader = UnstructuredPDFLoader('./layoutparser_paper.pdf')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "KTQxzXiyVgj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Microsoft Office Document Loaders**\n",
        "The Microsoft Office suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n",
        "\n",
        "Unstructured.io provides a variety of document loaders to load MS Office documents. Check them out here.\n",
        "\n",
        "Here we will leverage LangChain's UnstructuredWordDocumentLoader to load data from a MS Word document."
      ],
      "metadata": {
        "id": "XhgCoRVoWqO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u189q_GVVrX0",
        "outputId": "26100674-2c2a-4b9f-8773-eadf1062e827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-\n",
            "To: /content/Quantum Computing.docx\n",
            "\r  0% 0.00/11.4k [00:00<?, ?B/s]\r100% 11.4k/11.4k [00:00<00:00, 28.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load word doc as a single document\n",
        "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
        "\n",
        "loader = UnstructuredWordDocumentLoader('./Quantum Computing.docx')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "KiEwuNcgWvow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU4cWtksW3tR",
        "outputId": "da2266b3-23e0-4212-d657-315fcb0cab70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0].page_content[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "kBr7yCSUXGdF",
        "outputId": "b8c90474-2884-4292-bece-5647e79afb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Rise of Quantum Computing: A New Era of Innovation\\n\\nFor decades, classical computing has driven technological advancements, but the limitations of traditional binary processing are becoming evident as the world demands more computational power. Enter quantum computing—a revolutionary approach that leverages the principles of quantum mechanics to solve complex problems at unprecedented speeds.\\n\\nUnderstanding Quantum Computing\\n\\nUnlike classical computers that process information using bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously due to superposition. This unique property allows quantum systems to process vast amounts of data in parallel, making them exponentially more powerful for specific tasks.\\n\\nAnother key principle, entanglement, enables qubits to be interconnected, meaning the state of one qubit is dependent on another, regardless of distance. This drastically enhances processing efficiency and speed, paving the way for breakt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load word doc with complex parsing and section based chunks\n",
        "\n",
        "loader = UnstructuredWordDocumentLoader('./Quantum Computing.docx',\n",
        "                                        strategy='fast',\n",
        "                                        chunking_strategy=\"by_title\",\n",
        "                                        max_characters=3000, # max limit of a document chunk\n",
        "                                        new_after_n_chars=2500, # preferred document chunk size\n",
        "                                        mode='elements')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "j0EFbwLEXdPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Fvn2JYXm3o",
        "outputId": "15ffb49d-72ab-4ec0-fe2e-f061b1f8d1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urmmlQvRXpKj",
        "outputId": "b4799979-62d8-4a16-9743-2e36ad6c98b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': './Quantum Computing.docx', 'emphasized_text_contents': ['Understanding Quantum Computing', 'qubits', 'superposition', 'entanglement', 'Applications Transforming Industries', 'Drug Discovery & Healthcare', 'Financial Modeling', 'Cybersecurity & Cryptography', 'post-quantum cryptography', 'Climate Modeling & Sustainability', 'AI & Machine Learning'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b'], 'file_directory': '.', 'filename': 'Quantum Computing.docx', 'languages': ['eng'], 'last_modified': '2025-02-19T14:45:48', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'ebbfc67ada357945d491043bfb0293aa'}, page_content='The Rise of Quantum Computing: A New Era of Innovation\\n\\nFor decades, classical computing has driven technological advancements, but the limitations of traditional binary processing are becoming evident as the world demands more computational power. Enter quantum computing—a revolutionary approach that leverages the principles of quantum mechanics to solve complex problems at unprecedented speeds.\\n\\nUnderstanding Quantum Computing\\n\\nUnlike classical computers that process information using bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously due to superposition. This unique property allows quantum systems to process vast amounts of data in parallel, making them exponentially more powerful for specific tasks.\\n\\nAnother key principle, entanglement, enables qubits to be interconnected, meaning the state of one qubit is dependent on another, regardless of distance. This drastically enhances processing efficiency and speed, paving the way for breakthroughs in cryptography, materials science, and artificial intelligence.\\n\\nApplications Transforming Industries\\n\\nDrug Discovery & Healthcare\\nQuantum simulations can analyze molecular structures and interactions at an atomic level, accelerating drug discovery and personalized medicine. Companies like IBM and Google are already exploring quantum approaches to fight diseases like cancer and Alzheimer’s.\\n\\nFinancial Modeling\\nFinancial markets involve complex, unpredictable variables. Quantum algorithms can optimize portfolios, manage risk, and predict market fluctuations with greater accuracy than classical computing methods.\\n\\nCybersecurity & Cryptography\\nTraditional encryption methods rely on mathematical complexity, which quantum computers could break in seconds. This has sparked the rise of post-quantum cryptography, aimed at developing secure algorithms resistant to quantum attacks.\\n\\nClimate Modeling & Sustainability\\nQuantum computing can process massive climate datasets, helping scientists model climate change scenarios with greater precision. This will improve renewable energy optimization, disaster prediction, and environmental impact assessment.\\n\\nAI & Machine Learning\\nQuantum-enhanced AI models can process data faster, recognize patterns more efficiently, and revolutionize deep learning architectures, leading to advancements in robotics, automation, and natural language processing.')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Directory Loaders\n",
        "\n",
        "LangChain's [`DirectoryLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html) implements functionality for reading files from disk into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects."
      ],
      "metadata": {
        "id": "PHTO424xXzTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'Vision Transformers.pdf' 'https://arxiv.org/pdf/2010.11929.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWG1iX0uXrYa",
        "outputId": "e700e69f-312b-4e75-d9ab-4295b8ebcb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-28 16:29:43--  https://arxiv.org/pdf/2010.11929.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2010.11929 [following]\n",
            "--2025-06-28 16:29:43--  http://arxiv.org/pdf/2010.11929\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3743814 (3.6M) [application/pdf]\n",
            "Saving to: ‘Vision Transformers.pdf’\n",
            "\n",
            "Vision Transformers 100%[===================>]   3.57M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-06-28 16:29:43 (60.8 MB/s) - ‘Vision Transformers.pdf’ saved [3743814/3743814]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define and assign specific loaders which can be used by LangChain when processing the files for a specific file type. We follow this format\n",
        "\n",
        "loaders = {\n",
        "  'file_format_extension' : (LoaderClass, LoaderKeywordArguments)\n",
        "}\n",
        "Where:\n",
        "\n",
        "file_format_extension can be anything like .docx, .pdfetc.\n",
        "LoaderClass is a specific data loader like PyMuPDFLoader\n",
        "LoaderKeywordArguments are any specific keyword arguments which needs to be passed into that loader at runtime"
      ],
      "metadata": {
        "id": "D0fwKfy4YFTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to map file extensions to their respective loaders\n",
        "loaders = {\n",
        "    '.pdf': (PyMuPDFLoader, {}),\n",
        "    '.docx': (UnstructuredWordDocumentLoader, {'strategy': 'fast',\n",
        "                                              'chunking_strategy' : 'by_title',\n",
        "                                              'max_characters' : 3000, # max limit of a document chunk\n",
        "                                              'new_after_n_chars' : 2500, # preferred document chunk size\n",
        "                                              'mode' : 'elements'\n",
        "                                              })\n",
        "}"
      ],
      "metadata": {
        "id": "5RHdyyOhX8eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DirectoryLoader accepts a loader_cls argument, which defaults to UnstructuredLoader but we can pass our own loaders which we defined above in the loader_clsargument and any keyword args for the loader can be passed in the loader_kwargs argument.\n",
        "\n",
        "We can also show a progress bar by setting show_progress=True\n",
        "\n",
        "We can use the glob parameter to control which files to load based on file patterns\n",
        "\n",
        "Here we create two separate loaders to load files which are word documents and PDFs"
      ],
      "metadata": {
        "id": "bhbtdY1TYMVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "# Define a function to create a DirectoryLoader for a specific file type\n",
        "def create_directory_loader(file_type, directory_path):\n",
        "    return DirectoryLoader(\n",
        "        path=directory_path,\n",
        "        glob=f\"**/*{file_type}\",\n",
        "        loader_cls=loaders[file_type][0],\n",
        "        loader_kwargs=loaders[file_type][1],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "# Create DirectoryLoader instances for each file type\n",
        "pdf_loader = create_directory_loader('.pdf', './')\n",
        "docx_loader = create_directory_loader('.docx', './')\n",
        "\n",
        "# Load the files\n",
        "pdf_documents = pdf_loader.load()\n",
        "docx_documents = docx_loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYVwtr4pYJ2Y",
        "outputId": "64fc95be-6847-4add-d285-1dfdfc0aa240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  8.15it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 11.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pdf_documents)"
      ],
      "metadata": {
        "id": "dswVsdF1YQyj",
        "outputId": "979d8864-d9db-4cdd-9ef2-0a8936d183de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_documents[18]"
      ],
      "metadata": {
        "id": "-7DFo3pAYTMe",
        "outputId": "d87fc51c-0c7b-42c0-f54b-e7328cccd1aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': 'Vision Transformers.pdf', 'file_path': 'Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 2}, page_content='Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\\ntheir efﬁcient implementations – can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\\ntached to z0\\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ﬁne-tuning time.\\nPosition embeddings are added to the patch embeddings to retain positional information. We use\\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\\nsequence of embedding vectors serves as input to the encoder.\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n3')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docx_documents)"
      ],
      "metadata": {
        "id": "-dx_9MgMYVFa",
        "outputId": "35fa1fca-4419-48d3-a05a-ed79c3a17c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjBuriJmYXZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}